import os
os.environ["CUDA_VISIBLE_DEVICES"] = "6"
from typing import Dict, List, Optional, Tuple, Union
import json5
from tool import Tools
import sys
import torch
sys.path.append('/work/ruioliao/topo_agent/model_manage')
from infer_register_cl import ModelInfer_Cl
from transformers import AutoModelForCausalLM, AutoTokenizer
from react_template import*

class Agent:
    def __init__(self, model_name):
        self.model_name = model_name 
        self.tool = Tools()
        self.overall_prompt = self.build_prompt()
        self.load_model()
        
    def load_model(self):
        ## Load the model
        self.model_manager = ModelInfer_Cl()
        self.model_manager.load_model(self.model_name, dtype=torch.float16, device="auto") 
       
    def build_prompt(self):
        tool_descs, tool_names = [], []
        for tool in self.tool.toolConfig:
            tool_descs.append(TOOL_DESC.format(**tool))
            tool_names.append(tool['name_for_model'])
        tool_descs = '\n\n'.join(tool_descs)
        tool_names = ','.join(tool_names)
        overall_prompt = REACT_PROMPT.format(tool_descs=tool_descs, tool_names=tool_names)
        return overall_prompt
    
    def parse_latest_plugin_call(self, text):
        """
        Parse the text generated by the model and find the latest plugin call.
        """
        plugin_name, plugin_args = '', ''
        i = text.rfind('\nAction:')
        j = text.rfind('\nAction Input:')
        k = text.rfind('\nObservation:')
        if 0 <= i < j:  # If the text contains `Action` and `Action Input`
            if k < j:  # But does not contain `Observation`
                text = text.rstrip() + '\nObservation:'  # Add the Observation field
            k = text.rfind('\nObservation:')
            plugin_name = text[i + len('\nAction:') : j].strip()
            plugin_args = text[j + len('\nAction Input:') : k].strip()
            text = text[:k]
        return plugin_name, plugin_args, text
    
    def call_plugin(self, plugin_name, plugin_args):
        """
        Call the corresponding tool based on the parsed plugin name and arguments.
        """
        plugin_args = json5.loads(plugin_args)
        if plugin_name == 'Con-CDVAE_topo':
            return '\nObservation:' + str(self.tool.crystal_generation(**plugin_args))

    def text_completion(self, text):
        """
        Process the input text, perform model inference, and call the tool based on the model's response.
        """
        text = "\nQuestion:" + text
        
        # Model inference call
        response = self.model_manager.model_infer(text, sys_content = self.overall_prompt)
        
        # Parse the latest plugin call
        plugin_name, plugin_args, response = self.parse_latest_plugin_call(response)
        
        # Call the plugin and handle the return value
        if plugin_name:
            response += self.call_plugin(plugin_name, plugin_args)
        
        # Call the model again to complete the conversation (recursive handling)
        response = self.model_manager.model_infer(response, self.overall_prompt)
        return response

if __name__ == '__main__':
    agent = Agent('your model name')
    
    # Natural language question, asking the agent to call the relevant tool based on input information
    question = "Generate crystals with parameters n_cry=1, bg=3, fe=-1, n_atom=2, formula=None, topo_class=None"
    
    # Simulate processing the request
    response, his = agent.text_completion(question)
    print(response)


