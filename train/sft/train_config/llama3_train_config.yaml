# config.yaml
per_device_train_batch_size: 100
gradient_accumulation_steps: 1
logging_steps: 50
learning_rate: 0.0001
remove_unused_columns: false
adam_epsilon: 0.0001
num_train_epochs: 50
save_strategy: epoch
resume_from_checkpoint: /work/HUGGINGFACE/model/trained_model/llama3_8b_eval_q/checkpoint-4050
output_dir: /work/HUGGINGFACE/model/trained_model/llama3_8b_eval_q
logging_dir: /work/ruioliao/topo_agent/tensorboard/
report_to: tensorboard
# deepspeed: '/work/ruioliao/topo_agent/train/deepspeed_config/sft_config.json'



