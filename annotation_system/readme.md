## **Annotation System for Large Language Models**

The **Annotation System** is designed to improve **data quality** and support advanced model training, particularly for **Reinforcement Learning with Human Feedback (RLHF)** and other fine-tuning tasks. Inspired by the need for clean, high-quality, and preference-based datasets, this system integrates tools for **prompt evolution**, **output evaluation**, **text extraction**, and **ranking mechanisms**.

The system ensures that language models are aligned with **user expectations** and optimized for **complex tasks** by providing tools to iteratively improve data quality and structure.

---

## **Folder Structure**

The directory is organized as follows:

```plaintext
annotation_system/
│
├── evol_classify.py           # Classifies data quality by evolving prompts to enhance complexity
├── format_extract.py          # Extracts and structures text from unstructured outputs
├── quality_classify.py        # Evaluates the quality of outputs generated by LLMs
├── rlhf_rank_2.py             # Ranks two outputs for preference-based RLHF tasks
├── rlhf_rank_5.py             # Ranks five outputs for preference-based RLHF tasks
├── test/                      # Contains test json files for evaluation
└── readme.md                  # Main documentation for the annotation system
```

---

## **System Components and Purpose**

### **1. Prompt Evolution and Quality Enhancement**
- **`evol_classify.py`**  
   - **Purpose**: Automatically rewrite and evolve prompts to increase their **complexity** and **depth**.  
   - **Impact**: Enhances the quality of datasets, making them more challenging and effective for model training.

- **`quality_classify.py`**  
   - **Purpose**: Evaluates the outputs of LLMs against key metrics like **accuracy**, **clarity**, and **informativeness**.  
   - **Impact**: Filters and identifies **high-quality outputs**, ensuring only the best data is used for further tasks.

---

### **2. Text Extraction and Structuring**
- **`format_extract.py`**  
   - **Purpose**: Extracts specific, structured content from raw or unstructured model outputs.  
   - **Impact**: Prepares **clean datasets** for downstream tasks, improving efficiency during model evaluation or fine-tuning.

---

### **3. Preference-Based Ranking for RLHF**
The system supports **Reinforcement Learning with Human Feedback (RLHF)**, which is essential for aligning language models with user preferences:

- **`rlhf_rank_2.py`**  
   - **Purpose**: Compares and ranks **two outputs** to generate preference-based feedback.  
   - **Impact**: Facilitates RLHF by producing simple pairwise comparisons to optimize model alignment.

- **`rlhf_rank_5.py`**  
   - **Purpose**: Allows the ranking of **five outputs** for more nuanced and detailed preference data.  
   - **Impact**: Captures complex user preferences, enabling models to handle more sophisticated ranking tasks.

---

### **4. Testing and Evaluation**
- **`test/`**  
   - **Purpose**: Contains sample JSON files to test and evaluate scripts for prompt evolution, extraction, and ranking.

---

## **System Highlights**

1. **Data Quality Improvement**:  
   - Tools like **prompt evolution** and **output evaluation** ensure datasets are rich, diverse, and high-quality.

2. **Preference-Based Alignment**:  
   - Supports RLHF through output comparison and ranking, enabling models to better reflect user expectations.

3. **Text Structuring**:  
   - Cleans and organizes unstructured data for streamlined downstream usage.

4. **Scalability and Modularity**:  
   - Each script is independent and modular, allowing easy integration into various annotation workflows.

---
